{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Taxonomy Using Correlation Martrix Between Labels\n",
    "\n",
    "\n",
    "### Prior Work.\n",
    "\n",
    "In [1], the authors assumed (in the first part of their paper) that classes follow a (known) two-level hierarchy: there is a set of \"super-classes\", each associated with a set of actual sub-classes; see figure below. \n",
    "<img src=\"figs/graph.png\">\n",
    "Weights to classify super-classes ( $\\beta_k$ ) as well as those to classify actual classes ( $W_j$ ) are learned, and each $W_j$ is regularized around the $\\beta_k$ corresponding to its super-class, thus minimizing the loss function\n",
    "\n",
    "\\begin{equation}\n",
    "L = -\\log P(\\mathcal{Y}|\\mathcal{X},W,\\beta) +\\lambda_0 \\|\\theta\\|^2 + \\lambda \\| \\beta \\|^2 + \\lambda_2\\sum_j \\| W_j -\\beta_{k(j)} \\|^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta$ is other paramters. Also, it turns out that for every super-class $k$, $\\beta_k\\approx\\sum W_j$ where the summation is over the corresponding classes under the super-class $k$.\n",
    "\n",
    "\n",
    "### Our Idea\n",
    "\n",
    "The main idea is to automatize the construction of the label graphs. We want to use correlations between the scores of different classes to do this. Then, we psuh the classes that are \"closer\" twoards each other. This idea has several key elements.\n",
    "\n",
    "#### 1. A weighted graph of classes based on correlation\n",
    "\n",
    "Consider a wieghted graph in which each node is a class, and the weight of the edge between to nodes $i$ and $j$ is \n",
    "\n",
    "$$\n",
    "F_{ij} = Correlation \\big(W_i . \\phi(X) , W_j . \\phi(X)\\big)\n",
    "$$\n",
    "\n",
    "where $\\phi(x)$ is the feature vector of an input picture $x$, and the correlation is taken over the set of all pictures.  \n",
    "\n",
    "The assumption is that classes that are more \"similar\", have larger correlations. We have tested this on the following set up:\n",
    "\n",
    "- CIFAR100 data set: that consists of 100 classes, 20 \"super-classes\" each containing 5 classes;\n",
    "\n",
    "- A Resnet architecture with 32 layers. \n",
    "\n",
    "We then computed the correlation between all the classes, and plotted the histogram of correlations \"within\" and \"between\" classes of different super-classes. The result suggets that our intuition is correct.\n",
    "<img src=\"figs/correlations.png\">\n",
    "\n",
    "#### 2. Regularization Based on the graph\n",
    "\n",
    "Now, in our proposed loss function, we introduce regularization terms that tries to push weights corresponding to different classes towards the classes most similar to it. In fact, we define a loss function similar to above\n",
    "\n",
    "$$\n",
    "L = -\\log P(\\mathcal{Y}|\\mathcal{X},W,\\beta) +\\lambda_0 \\|\\theta\\|^2 + \\lambda \\| \\beta \\|^2 + \\lambda_2\\sum_j \\| W_j -\\beta_{k(j)} \\|^2.\n",
    "$$\n",
    "\n",
    "However, in our case, a $\\beta_j$ is defined for all classes as follows\n",
    "\n",
    "$$\n",
    "\\beta_j = \\sum_{N(j)} F_{ij} W_i ,\n",
    "$$\n",
    "\n",
    "where $N(j)$ is a subset of nodes that are most \"informative\" about the class $j$, and is TBD. Some possibilites are 1) all nodes, 2) nodes $i$ with correlations $F_{ij}$ above a certain threshold, or 3) nodes $i$ for which $|F_{i,j}|$ is above a certain treshold (to include significant negative correlations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Qustions, Ideas, Criticisms, etc.\n",
    "\n",
    "##### General \n",
    "\n",
    "- What are the metrics we want to improve? 1) accuracy of rare classes (exact or upto super-class level) 2) interpretability? 3) general accuracy?\n",
    "\n",
    "\n",
    "\n",
    "##### On correlation part:\n",
    "\n",
    "- should the correlation be taken on a subset of input pictures, for examples, only on those with certain labels.\n",
    "\n",
    "\n",
    "\n",
    "##### On Loss function\n",
    "\n",
    "- How should $N(j)$ be defined? Should we use methods in mentioned in [2] to make a block out of these fully connectd graph?\n",
    "\n",
    "- Should we regularize other parameters $\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
